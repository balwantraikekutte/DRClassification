import resnet

import numpy as np
from sklearn.metrics import cohen_kappa_score

import tensorflow as tf
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator

print('loading the dataset...')

trainingData = np.load('trainingData.npy')
trainingLabel = np.load('trainingLabel.npy')

print('Splitting into training and validation sets...')
trainData = trainingData[0:8000,:,:,:]
trainLabel = trainingLabel[0:8000,]

print('Training set created...')
validationData = trainingData[8000:9000,:,:,:]
validationLabel = trainingLabel[8000:9000,]

print('Validation set created...')
testData = np.load('testingData.npy')
testLabel = np.load('testingLabel.npy')

print('Test data loaded...')
testDataPreProcessed = np.load('testingData1.npy')
testLabelPreProcessed = np.load('testingLabel1.npy')
print('Pre processed test data loaded...')
print('')

print('Preprocessing the data...')
print('')

# mean = np.mean(trainData)  # mean for data centering
# std = np.std(trainData)  # std for data normalization
# trainData -= mean
# trainData /= std

# convert class vectors to binary class matrices
nb_classes = 5
trainLabel = np_utils.to_categorical(trainLabel, nb_classes)
validationLabel = np_utils.to_categorical(validationLabel, nb_classes)

print('Building and compiling the model...')
print('')

with tf.device('/gpu:0'):
    
    model = resnet.ResnetBuilder.build_resnet_18((3,512,512), 5)
    
    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
    
    data_augmentation = True
    
    if not data_augmentation:
        
        print('Not using data augmentation...')
        
        model.fit(trainData, trainLabel, batch_size=16, nb_epoch=200, validation_split=0.2, shuffle=True)
    
    else:
        
        print('Using real-time data augmentation...')
        
        # This will do preprocessing and realtime data augmentation:
        datagen = ImageDataGenerator(
                featurewise_center=False,  # set input mean to 0 over the dataset
                samplewise_center=False,  # set each sample mean to 0
                featurewise_std_normalization=False,  # divide inputs by std of the dataset
                samplewise_std_normalization=False,  # divide each input by its std
                zca_whitening=False,  # apply ZCA whitening
                rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)
                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
                horizontal_flip=True,  # randomly flip images
                vertical_flip=False)  # randomly flip images
        
        # Compute quantities required for featurewise normalization
        # (std, mean, and principal components if ZCA whitening is applied).
        
        datagen.fit(trainData)
        
        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(datagen.flow(trainData, trainLabel, batch_size=16), steps_per_epoch=trainData.shape[0] // 16, epochs=150, validation_data=(validationData,validationLabel), verbose=1)

model.save('resnet18.h5')

print('testing the model...')
pred = model.predict(testData, batch_size=16)
pred = np.argmax(pred, axis=1)
test = np.argmax(testLabel, axis=1)
cohen_kappa = cohen_kappa_score(test, pred)
print('Kappa score for the model on test set is', cohen_kappa)
print('')

print('testing the model with pre processed test data...')
predPreProcessed = model.predict(testDataPreProcessed, batch_size=16)
predPreProcessed = np.argmax(predPreProcessed, axis=1)
testPreProcessed = np.argmax(testLabelPreProcessed, axis=1)
cohen_kappa_pre_processed = cohen_kappa_score(testPreProcessed, pred)
print('Kappa score for the model on the preprocessed test set is',cohen_kappa_pre_processed)
