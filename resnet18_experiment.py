import resnet

import numpy as np
from sklearn.metrics import cohen_kappa_score

import tensorflow as tf
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator

print('loading the dataset...')

trainData1 = np.load('immatrix3.npy')
trainLabel1 = np.load('imlabel3.npy')

trainData = trainData1[0:8000,:,:,:]
trainLabel = trainLabel1[0:8000,]

testData = trainData1[9000:10000,:,:,:]
testLabel = trainLabel1[9000:10000,]

print('Preprocessing the data...')

# mean = np.mean(trainData)  # mean for data centering
# std = np.std(trainData)  # std for data normalization
# trainData -= mean
# trainData /= std

# convert class vectors to binary class matrices
nb_classes = 5
trainLabel = np_utils.to_categorical(trainLabel, nb_classes)


print('Building and compiling the model...')

with tf.device('/gpu:0'):
    
    model = resnet.ResnetBuilder.build_resnet_18((3,512,512), 5)
    
    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
    
    data_augmentation = True
    
    if not data_augmentation:
        
        print('Not using data augmentation...')
        
        model.fit(trainData, trainLabel, batch_size=16, nb_epoch=200, validation_split=0.2, shuffle=True)
    
    else:
        
        print('Using real-time data augmentation...')
        
        # This will do preprocessing and realtime data augmentation:
        datagen = ImageDataGenerator(
                featurewise_center=False,  # set input mean to 0 over the dataset
                samplewise_center=False,  # set each sample mean to 0
                featurewise_std_normalization=False,  # divide inputs by std of the dataset
                samplewise_std_normalization=False,  # divide each input by its std
                zca_whitening=False,  # apply ZCA whitening
                rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)
                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
                horizontal_flip=True,  # randomly flip images
                vertical_flip=False)  # randomly flip images
        
        # Compute quantities required for featurewise normalization
        # (std, mean, and principal components if ZCA whitening is applied).
        
        datagen.fit(trainData)
        
        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(datagen.flow(trainData, trainLabel, batch_size=64), steps_per_epoch=trainData.shape[0] // 16, epochs=150, verbose=1, max_q_size=100)

print('testing the model...')
pred = model.predict(testData, batch_size=64)
pred = np.argmax(pred, axis=1)
test = np.argmax(testLabel, axis=1)
cohen_kappa = cohen_kappa_score(test, pred)

